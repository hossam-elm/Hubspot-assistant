from openai import OpenAI
from auths.auth import get_google_sheet
import os
from dotenv import load_dotenv
from datetime import datetime, timezone
from pipe import report
from typing import Dict, List
import time
from gspread.exceptions import APIError
from utils.setup_log import logger
import tiktoken
from collections import deque


load_dotenv()

# Keys
serpapi_key = os.getenv('serpapi_key')
google_key = os.getenv('google_key')
SERVICE_ACCOUNT_FILE = os.getenv('SERVICE_ACCOUNT_FILE')
google_cse_id = os.getenv('google_cse_id')
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
hubspot_key = os.getenv('hubspot_key')
sheet_key = os.getenv('sheet_key')


client = OpenAI()
TPM_LIMIT = 200_000
TOKEN_WINDOW_SECONDS = 60
token_timestamps = deque()

encoding = tiktoken.encoding_for_model("gpt-4o-mini")
def count_tokens(text: str) -> int:
    return len(encoding.encode(text))
def throttle_if_needed(tokens_needed: int):
    now = time.time()
    # Remove tokens older than 60 seconds
    while token_timestamps and now - token_timestamps[0][0] > TOKEN_WINDOW_SECONDS:
        token_timestamps.popleft()

    used_tokens = sum(t[1] for t in token_timestamps)

    if used_tokens + tokens_needed > TPM_LIMIT:
        wait_time = TOKEN_WINDOW_SECONDS - (now - token_timestamps[0][0])
        logger.warning(f"‚è≥ Token limit reached. Waiting {int(wait_time)+1}s...")
        time.sleep(wait_time + 1)

    token_timestamps.append((time.time(), tokens_needed))

def truncate_web_context(semantic_ctx, max_tokens=50000):
    truncated_chunks = []
    total_tokens = 0

    for item in semantic_ctx:
        chunk_text = f"‚Ä¢ [{item['chunk_id']}] {item['chunk']} (‚Ü™ {item['link']})"
        chunk_tokens = count_tokens(chunk_text)

        if total_tokens + chunk_tokens > max_tokens:
            # Stop adding more chunks when token limit is reached
            break
        truncated_chunks.append(chunk_text)
        total_tokens += chunk_tokens

    return "\n".join(truncated_chunks)

def get_company_profile(company_name: str) -> Dict[str, str]:
    try:
        rpt = report(company_name)
        semantic_ctx = rpt["semantic_items"]
        linkedin_profiles = rpt["linkedin_profiles"]
    except Exception as e:
        logger.error(f"[ERROR] report() failed for {company_name}: {e}", exc_info=True)
        semantic_ctx = []
        linkedin_profiles = []

    # Build semantic context for prompt
    if semantic_ctx:
        web_context = truncate_web_context(semantic_ctx, max_tokens=50000)
        if not web_context.strip():
            web_context = "Aucune information fiable trouv√©e."
    else:
        web_context = "Aucune information fiable trouv√©e."
    # 2) Dedupe linkedin_profiles by name or link
    seen = set()
    unique_profiles = []
    for p in linkedin_profiles:
        key = (p['name'].lower(), p['link'].lower())
        if key not in seen:
            seen.add(key)
            unique_profiles.append(p)

    # 3) Rank LinkedIn contacts: Office > HR > Marketing > Talent Acquisition > others
    def rank_key(profile):
        job = profile.get('job', '').lower()
        if 'cse' in job:
            return 1
        if 'office' in job:
            return 2
        if 'hr' in job or 'human resources' in job:
            return 3
        if 'people' in job:
            return 4
        if 'marketing' in job:
            return 5
        if 'talent acquisition' in job or 'talent ' in job:
            return 6
        return 7

    unique_profiles.sort(key=rank_key)

    # 4) Build the LinkedIn block from ranked unique_profiles
    if unique_profiles:
        li_block = "\n".join(
            f"‚Ä¢ {p['name']} ‚Äì {p.get('job','N/A')} ‚Äì {p['link']}"
            for p in unique_profiles
        )
    else:
        li_block = "Aucun contact Linkedin trouv√©."

    # --- Build prompt WITHOUT the interlocuteurs section ---
    prompt = f"""
Tu r√©ponds en fran√ßais, tu priorises les informations r√©centes (2025) et les chiffres pr√©cis, tu es un analyste B2B senior chez Atelier Box, expert en cadeaux d‚Äôentreprise personnalis√©s, welcome packs, textiles premium, goodies √©coresponsables et e-shops internes.

Utilise ce contexte web (extraits s√©mantiques) pour g√©n√©rer la fiche compte :
{web_context}

üè¢ FICHE COMPTE ‚Äì {company_name}
üîπ Secteur & Mod√®le √©conomique
‚Ä¢ Secteur d‚Äôactivit√© (ex : FinTech, SaaS, Retail‚Ä¶)
‚Ä¢ Mod√®le √©conomique (ex : abonnement B2B, marketplace‚Ä¶)
‚Ä¢ Date de cr√©ation ou si l'entreprise est le r√©sultat de fusion tu donne cette date (format JJ/MM/YYYY, faut donner la date compl√®te)  + age en 2025 + source

üë• Taille de l‚Äôentreprise
‚Ä¢ Nombre de collaborateurs (total + France si dispo) si tu ne sais pas tu donne une estimation + source de l'info
‚Ä¢ √âvolution : hypercroissance / stable / d√©croissance

üí∞ Clients / Chiffre d'affaires
‚Ä¢ Nombre estim√© de clients si tu ne sais pas tu donne une estimation
‚Ä¢ Chiffre d‚Äôaffaires annuel, si tu ne sais pas tu donne une estimation chiffr√©es

üåç Pr√©sence g√©ographique
‚Ä¢ Si√®ge social
‚Ä¢ Autres bureaux (villes + pays) + nombre total
‚Ä¢ nombre de pays ou l'entreprise est pr√©sente

üèõÔ∏è CSE
‚Ä¢ Pr√©sence d‚Äôun CSE ? (Oui / Non / √Ä confirmer) si tu ne sais pas tu donnes une estimation et justification

üî• Actualit√©s & signaux business
‚Ä¢ Lev√©e de fonds avec dates et s√©rie / rebranding avec dates / date et localisation lancement de bureaux
‚Ä¢ Recrutement actif? tu donnes le nombre de poste ouverts
‚Ä¢ Expansion produit ou g√©ographique avec dates/ann√©es
‚Ä¢ Rebranding, achats/fusion, positionnement, tu donnes le nom de la campagne et la dates/ann√©es
‚Ä¢ action relative √† la certification RSE, expansion, croissance avec des dates pr√©cises 

üéØ Opportunit√©s Atelier Box:
Voici des exemples d‚Äôopportunit√©s que nous proposons :  
  ‚Äì Welcome kits onboarding  
  ‚Äì Textiles internes (hoodies, polos, vestes)  
  ‚Äì Goodies pour √©v√©nements ou packs d√©mo  
  ‚Äì Coffrets VIP (speakers, direction)  
  ‚Äì Boxes culture interne (anniversaires, milestones)  
  ‚Äì E-shop marque blanche (multi-pays) 
  √Ä partir du contexte web, g√©n√®re **au moins 3** opportunit√©s **sp√©cifiquement personnalis√©es** pour {company_name}. Donne pour chaque :
    1. Un titre court (emoji optionnel)  
    2. Une phrase expliquant pourquoi c‚Äôest pertinent  
    3. Une estimation de fr√©quence ou volume si possible
  finalement tu donne un petit brief pour nos commercials qui r√©sume tous √ßa pour savois exactement comment int√©ragir avec ce lead

üìÖ Calendrier opportunit√©s 2025 (sections tr√®s importantes)
  ‚Ä¢ Liste exhaustive des √©v√©nements internes ou publics √† venir, sinon les √©v√©nements pass√©s en 2025, pour r√©f√©rence on est on mai 2025 avec **dates pr√©cises** (JJ/MM/AAAA) : noms, lieux et objectifs pour chaque, au moins 5


‚úÖ Score Atelier Box
En vous basant sur le potentiel de cette entreprise en mati√®re de cadeaux d‚Äôentreprise, veuillez attribuer un score sur 100 en utilisant la grille d‚Äô√©valuation en bas, tu te base sur la croissance, nombre de collaborateurs, √©v√©nements, cse et autres indices que tu pense importants
90‚Äì100: Strategic key account: CSE, pr√©sence dans plusieurs pays, +1000 collaborateurs..

80‚Äì89: High-priority:  CSE, pr√©sence dans plusieurs pays, +500 collaborateurs...

70‚Äì79: Warm lead: +200 collaborateurs

60‚Äì69: Nurture/monitor: +100 collaborateurs

<60: Low match: the rest

. √âvitez de donner syst√©matiquement la m√™me note de 85 ou autres, je t'encourages √† donner des mauvaise note pour ceux qui ne m√©rite pas, think critically et d'habitude on cherche un client pour au moins 1500 euros de commande, la box co√ªte au moins 45 euros et tu sais d√©ja le prix pour textiles ou objets. Soyez rigoureux et baissez la note en cas de donn√©es manquantes ou faibles.
‚Ä¢ Justification en une phrase claire

Entreprise √† analyser : {company_name}
"""
    input_tokens = count_tokens(prompt)
    throttle_if_needed(input_tokens)
    completion = client.chat.completions.create(
        model="gpt-4o-mini",
        temperature=0.0,
        top_p=1,
        presence_penalty=0, 
        frequency_penalty=0,
        messages=[
            {"role": "developer", "content": "You are a company profile generator for Atelier Box, you prioritize numbers and dates, and you give actionable and concrete informations."},
            {"role": "user",      "content": prompt}
        ]
    )
    
    profile_text = completion.choices[0].message.content
    output_tokens = count_tokens(profile_text)
    throttle_if_needed(output_tokens)
    logger.info(f"üî¢ Input tokens: {input_tokens}, Output tokens: {output_tokens}")

    # Return both the profile and the Linkedin contacts block
    return {
        "profile": profile_text,
        "linkedin_contacts": li_block
    }


BATCH_SIZE = 5

def batch_update_with_fallback(sheet, data):
    for attempt in range(5):
        try:
            sheet.batch_update(data)
            return True
        except APIError as e:
            err_str = str(e).lower()
            if "500" in err_str or "timeout" in err_str:
                wait_time = 2 ** attempt
                logger.error(f"‚ö†Ô∏è Batch API error (attempt {attempt + 1}) ‚Äî retrying in {wait_time}s...")
                time.sleep(wait_time)
            else:
                logger.error(f"‚ùå Unrecoverable batch update error: {e}")
                break
    return False

def fill_bd():
    sheet = get_google_sheet(SERVICE_ACCOUNT_FILE, sheet_key)
    records = sheet.get_all_records()
    names = sheet.col_values(2)
    existing_profiles = sheet.col_values(3)

    logger.info(f"üìã Found {len(records)} total rows in sheet")
    
    # Count companies that need processing
    companies_to_process = 0
    for i, row in enumerate(records, start=2):
        company_name = names[i - 1] if i - 1 < len(names) else ""
        current_note = existing_profiles[i - 1] if i - 1 < len(existing_profiles) else ""
        if company_name.strip() and not current_note.strip():
            companies_to_process += 1
    
    logger.info(f"üéØ Found {companies_to_process} companies that need profiles generated")
    
    if companies_to_process == 0:
        logger.info("‚úÖ All companies already have profiles - nothing to do!")
        return

    batch_updates: List[tuple[int, str]] = []
    processed_count = 0

    for i, row in enumerate(records, start=2):
        company_name = names[i - 1] if i - 1 < len(names) else ""
        current_note = existing_profiles[i - 1] if i - 1 < len(existing_profiles) else ""

        if not company_name.strip() or current_note.strip():
            continue

        logger.info(f"üîç Generating profile for: {company_name}")
        result = get_company_profile(company_name)
        if not result:
            logger.warning(f"‚ö†Ô∏è Failed to generate profile for: {company_name}")
            continue

        linkedin_contacts = result.get('linkedin_contacts', '')
        if isinstance(linkedin_contacts, list):
            linkedin_contacts = "\n".join(linkedin_contacts)

        combined = f"{result['profile']}\n\nüë• Interlocuteurs cl√©s √† contacter\n{linkedin_contacts}".strip()
        batch_updates.append((i, combined))
        processed_count += 1
        logger.info(f"‚úÖ Profile ready for row {i} ({company_name}) - {processed_count}/{companies_to_process} completed")

        # As soon as we have 10 updates, send them in one batch
        if len(batch_updates) >= BATCH_SIZE:
            data = [
                {'range': f'C{row_index}', 'values': [[text]]}
                for row_index, text in batch_updates
            ]
            success = batch_update_with_fallback(sheet, data)
            if success:
                logger.info(f"‚úÖ Batch of {len(batch_updates)} rows updated.")
            else:
                logger.error(f"‚ö†Ô∏è Batch update failed for rows {batch_updates[0][0]}‚Äì{batch_updates[-1][0]}. Trying individually.")
                for row_index, text in batch_updates:
                    for attempt in range(5):
                        try:
                            sheet.update(values=[[text]], range_name=f'C{row_index}')
                            logger.info(f"‚úÖ Updated row {row_index} individually.")
                            break
                        except APIError as e:
                            err_str = str(e).lower()
                            if "500" in err_str or "timeout" in err_str:
                                wait_time = 2 ** attempt
                                logger.warning(f"‚ö†Ô∏è API error on row {row_index} (attempt {attempt+1}) ‚Äî retrying in {wait_time}s...")
                                time.sleep(wait_time)
                            else:
                                logger.error(f"‚ùå Unrecoverable error on row {row_index}: {e}")
                                break

            # Clear the batch list and continue
            batch_updates.clear()

    # After the loop, if any updates remain (< BATCH_SIZE), send them as a final batch
    if batch_updates:
        data = [
            {'range': f'C{row_index}', 'values': [[text]]}
            for row_index, text in batch_updates
        ]
        success = batch_update_with_fallback(sheet, data)
        if success:
            logger.info(f"‚úÖ Final batch of {len(batch_updates)} rows updated.")
        else:
            logger.error(f"‚ö†Ô∏è Final batch update failed for rows {batch_updates[0][0]}‚Äì{batch_updates[-1][0]}. Trying individually.")
            for row_index, text in batch_updates:
                for attempt in range(5):
                    try:
                        sheet.update(values=[[text]], range_name=f'C{row_index}')
                        logger.info(f"‚úÖ Updated row {row_index} individually.")
                        break
                    except APIError as e:
                        err_str = str(e).lower()
                        if "500" in err_str or "timeout" in err_str:
                            wait_time = 2 ** attempt
                            logger.warning(f"‚ö†Ô∏è API error on row {row_index} (attempt {attempt+1}) ‚Äî retrying in {wait_time}s...")
                            time.sleep(wait_time)
                        else:
                            logger.error(f"‚ùå Unrecoverable error on row {row_index}: {e}")
                            break

    logger.info("‚úÖ All profiles processed and written.")


if __name__ == "__main__":
    logger.info("üöÄ Starting HubSpot Assistant - Company Profile Generator")
    logger.info(f"üìä Processing Google Sheet: {sheet_key}")
    logger.info(f"üîë Using OpenAI model: gpt-4o-mini")
    
    try:
        fill_bd()
        logger.info("üéâ HubSpot Assistant completed successfully!")
    except Exception as e:
        logger.error(f"üí• HubSpot Assistant failed: {e}")
        raise